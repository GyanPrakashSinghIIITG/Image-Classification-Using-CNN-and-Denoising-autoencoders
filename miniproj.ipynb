{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"miniproj.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"ZsV8Ce_bdj9w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1227},"outputId":"92e5dd58-85c3-4b77-a5ab-b4133ba29538","executionInfo":{"status":"ok","timestamp":1555359637062,"user_tz":-330,"elapsed":285253,"user":{"displayName":"Gyan prakash singh","photoUrl":"https://lh6.googleusercontent.com/-cwsnOyfCsSY/AAAAAAAAAAI/AAAAAAAAAJU/zEtZ1f4ZPF0/s64/photo.jpg","userId":"08763604823469097634"}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.svm import SVC \n","from scipy.ndimage import convolve\n","from sklearn import linear_model, datasets, metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import BernoulliRBM\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import clone\n","\n","from keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","from keras.models import Model\n","from imgaug import augmenters\n","import pandas as pd\n","import keras\n","\n","\n","from __future__ import print_function, division\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","plt.ion()  \n","\n","!pip install split-folders\n","!rm -rf output\n","\n","!git clone https://github.com/pruvi007/ML_Datasets.git\n","    \n","import split_folders\n","split_folders.ratio('ML_Datasets/UCMerced_LandUse/Images', output=\"output\", seed=1337, ratio=(.8, .2)) \n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(256),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(256),\n","        transforms.CenterCrop(256),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","data_dir = 'output'\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n","                                          data_transforms[x])\n","                  for x in ['train', 'val']}\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=32,\n","                                             shuffle=True, num_workers=4)\n","              for x in ['train', 'val']}\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","save_file_name = 'alexnet-transfer.pt'\n","#len(class_names)\n","\n","model = torchvision.models.alexnet(pretrained=True)\n","\n","from torchsummary import summary\n","model.features\n","#summary(model.features.cuda(), (3,256,256))\n","\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","model = model.features.to(device)\n","\n","batch_size=32\n","train_size, validation_size = 1680, 420\n","def extract_features(phase, sample_count):\n","    features = np.zeros(shape=(sample_count, 256, 7, 7))  # Must be equal to the output of the convolutional base\n","    labels = np.zeros(shape=(sample_count))\n","   \n","    # Pass data through convolutional base\n","    i = 0\n","    for inputs, label in dataloaders[phase]:\n","        inputs = inputs.to(device)\n","        features_batch = model(inputs)\n","        features[i * batch_size: (i + 1) * batch_size] = features_batch.cpu().numpy()\n","        labels[i * batch_size: (i + 1) * batch_size] = label.numpy()\n","        i += 1\n","        if i * batch_size >= sample_count:\n","            break\n","    return features, labels\n","  \n","train_features, train_labels = extract_features('train', train_size) \n","validation_features, validation_labels = extract_features('val', validation_size)\n","\n","def scale(X, eps = 0.001):\n","\t# scale the data points s.t the columns of the feature space\n","\t# (i.e the predictors) are within the range [0, 1]\n","\treturn (X - np.min(X, axis = 0)) / (np.max(X, axis = 0) + eps)\n","\n","X_train, y_train = train_features.reshape(1680,7*7*256), train_labels\n","X_train = scale(X_train)\n","X_test = scale(validation_features.reshape(420,7*7*256))\n","y_test = validation_labels\n","X_test.shape\n","\n","import keras\n","y_train = keras.utils.to_categorical(y_train,21)\n","y_test = keras.utils.to_categorical(y_test,21)\n","\n","## input layer\n","input_layer = Input(shape=(12544,))\n","\n","## encoding architecture\n","encode_layer1 = Dense(200, activation='relu')(input_layer)\n","# encode_layer2 = Dense(5000, activation='relu')(encode_layer1)\n","# encode_layer3 = Dense(500, activation='relu')(encode_layer2)\n","\n","## latent view\n","latent_view   = Dense(12544, activation='sigmoid')(encode_layer1)\n","\n","## decoding architecture\n","# decode_layer1 = Dense(500, activation='relu')(latent_view)\n","# decode_layer2 = Dense(5000, activation='relu')(decode_layer1)\n","# decode_layer3 = Dense(15000, activation='relu')(decode_layer2)\n","\n","## output layer\n","output_layer  = Dense(21, activation='softmax')(latent_view)\n","\n","autoencoder = Model(input_layer,latent_view)\n","\n","autoencoder.compile(optimizer='adam', loss='mse')\n","# early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n","# autoencoder.fit(principalDf, principalDf, epochs=20, batch_size=32, validation_data=(principalDf1,principalDf1 ),verbose=1)\n","autoencoder.fit(scale(X_train), scale(X_train), epochs=10, batch_size=32, validation_data=(scale(X_test), scale(X_test) ),verbose=1)\n","\n","model = Model(input_layer, output_layer)\n","model.compile(loss=keras.losses.categorical_crossentropy,optimizer='adam',metrics=['accuracy'])\n","model_log = model.fit(scale(X_train), y_train, epochs=20, batch_size=32, validation_data=(scale(X_test), y_test ),verbose=1)\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: split-folders in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","fatal: destination path 'ML_Datasets' already exists and is not an empty directory.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n","Train on 1680 samples, validate on 420 samples\n","Epoch 1/10\n","1680/1680 [==============================] - 7s 4ms/step - loss: 0.0398 - val_loss: 0.0254\n","Epoch 2/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0160 - val_loss: 0.0251\n","Epoch 3/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0158 - val_loss: 0.0249\n","Epoch 4/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0156 - val_loss: 0.0247\n","Epoch 5/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0155 - val_loss: 0.0245\n","Epoch 6/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0153 - val_loss: 0.0244\n","Epoch 7/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0152 - val_loss: 0.0242\n","Epoch 8/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0150 - val_loss: 0.0240\n","Epoch 9/10\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0149 - val_loss: 0.0239\n","Epoch 10/10\n","1680/1680 [==============================] - 6s 3ms/step - loss: 0.0147 - val_loss: 0.0237\n","Train on 1680 samples, validate on 420 samples\n","Epoch 1/20\n","1680/1680 [==============================] - 7s 4ms/step - loss: 2.1241 - acc: 0.3476 - val_loss: 1.3140 - val_acc: 0.5524\n","Epoch 2/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 1.0010 - acc: 0.6411 - val_loss: 0.8496 - val_acc: 0.6857\n","Epoch 3/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.5105 - acc: 0.8298 - val_loss: 0.6201 - val_acc: 0.7738\n","Epoch 4/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.2159 - acc: 0.9375 - val_loss: 0.5500 - val_acc: 0.8381\n","Epoch 5/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0880 - acc: 0.9798 - val_loss: 0.5127 - val_acc: 0.8381\n","Epoch 6/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0278 - acc: 0.9976 - val_loss: 0.5191 - val_acc: 0.8548\n","Epoch 7/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.5029 - val_acc: 0.8714\n","Epoch 8/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.4977 - val_acc: 0.8667\n","Epoch 9/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.5036 - val_acc: 0.8667\n","Epoch 10/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.5141 - val_acc: 0.8762\n","Epoch 11/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8714\n","Epoch 12/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.5291 - val_acc: 0.8810\n","Epoch 13/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 8.1621e-04 - acc: 1.0000 - val_loss: 0.5324 - val_acc: 0.8810\n","Epoch 14/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 6.4193e-04 - acc: 1.0000 - val_loss: 0.5444 - val_acc: 0.8810\n","Epoch 15/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 5.1455e-04 - acc: 1.0000 - val_loss: 0.5466 - val_acc: 0.8810\n","Epoch 16/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 4.1950e-04 - acc: 1.0000 - val_loss: 0.5592 - val_acc: 0.8714\n","Epoch 17/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 3.5184e-04 - acc: 1.0000 - val_loss: 0.5546 - val_acc: 0.8762\n","Epoch 18/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 3.0018e-04 - acc: 1.0000 - val_loss: 0.5619 - val_acc: 0.8738\n","Epoch 19/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 2.5363e-04 - acc: 1.0000 - val_loss: 0.5632 - val_acc: 0.8762\n","Epoch 20/20\n","1680/1680 [==============================] - 6s 4ms/step - loss: 2.2258e-04 - acc: 1.0000 - val_loss: 0.5670 - val_acc: 0.8786\n"],"name":"stdout"}]}]}